{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c39951",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from tectosaur2.nb_config import setup\n",
    "\n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505ee88",
   "metadata": {},
   "source": [
    "# QBX examples for the Laplace equation: fun with screw dislocations\n",
    "\n",
    "In the last section, I introduced the basic concepts behind quadrature by expansion, put together a simple implementation, demonstrated that it works well and then finished by demonstrating `tectosaur2.integrate_term` as a more robust and complete implementation. This time, we'll use `integrate_term` to make some pretty pictures that might have some physical meaning.\n",
    "\n",
    "## Antiplane shear\n",
    "\n",
    "A basic result in linear elastic earthquake modeling is the representation of displacement from slip on an infinitely long strike-slip fault. Because the fault is infinitely long, all displacements is fault parallel. Suppose we're looking at a cross-section in the $x,y$ plane with all displacement occuring in the $z$ direction. Then the displacement vector is $\\mathbf{u} = (0, 0, u_z)$. And the strain state is:\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\varepsilon} = \\begin{bmatrix}\n",
    "0 & 0 & \\epsilon_{13} \\\\\n",
    "0 & 0 & \\epsilon_{23}\\\\\n",
    " \\epsilon_{13}    &    \\epsilon_{23}      & 0\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This special state is called \"antiplane shear\". We can simplify the equations of linear elasticity to be in terms of a vector strain, $\\epsilon_z = (\\epsilon_{xz}, \\epsilon_{yz})$ and vector stress, $\\sigma_z = (2\\mu\\epsilon_{xz}, 2\\mu\\epsilon_{yz})$. Combined with Newtons law, we get the result that $u_z$ is a solution to the Laplace equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^2 u_z = 0\n",
    "\\end{equation}\n",
    "\n",
    "As a result, we can describe the elastic behavior of infinitely long strike-slip faults (aka a \"screw dislocation\") in terms of solutions to the Laplace equation.\n",
    "\n",
    "Below, we're going to use QBX to compute the displacements and stresses resulting from slip on infinitely long strike-slip faults with fun shapes. In particular, the \"double layer\" integral we computed in part 1 will compute displacement in the volume from the input slip on the fault. We'll also introduce the \"hypersingular\" integral to calculate stresses from slip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d976e2",
   "metadata": {},
   "source": [
    "There are three missing pieces before we're ready to compute displacements and stresses from arbitrarily shaped faults:\n",
    "1. A mechanism for defining the fault mesh. We're going to introduce some tools for creating a boundary mesh from a symbolically parameterized curve. \n",
    "2. Tools for interpolating on that fault mesh. This is actually entirely hidden inside the functions we're using but it's still nice to introduce since it's conceptually very important.\n",
    "3. The hypersingular integral for computing stresses given fault slip. Previously, we've computed displacement given fault slip using the double layer potential. By analytically differentiating the double layer kernel, we can derive a new integral that computes stress!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed50696",
   "metadata": {},
   "source": [
    "## Meshing a symbolic surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c11454",
   "metadata": {},
   "source": [
    "In the last section, we integrated over a circle. Circles are simple so we didn't need much in the way of mesh handling code! But now, we're going to need a little bit more. In particular, it will be nice to separate our surface into several \"panels\". Panels are analogous to elements in a finite element method but make it clear that we're talking about a surface discretization and not a volumetric discretization.  \n",
    "\n",
    "\n",
    "The main tool we're going to use is the `tectosaur2.panelize_symbolic_surface` function. This function processes a set of symbolic surfaces into a form that is ready for integration. Internally, `panelize_symbolic_surface` segments the symbolic curve and then discretized it according to the quadrature rule we provide. The discretized points, normals and jacobians are computed and stored in the returned `PanelSurface` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e8457",
   "metadata": {},
   "source": [
    "Let's try this out with a simple curve:\n",
    "\\begin{align}\n",
    "x &= t\\\\\n",
    "y &= \\cos(t)\n",
    "\\end{align}\n",
    "for $t \\in [-1, 1]$\n",
    "\n",
    "I'll plot the surface along with the normal vectors. The normal vectors will be colored to show the determinant of the Jacobian. We can see that the Jacobian is slightly larger at the ends of the surface than the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da735aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from tectosaur2 import panelize_symbolic_surface, gauss_rule\n",
    "\n",
    "# The definition of the parameterized surface.\n",
    "sp_t = sp.var(\"t\")\n",
    "x = sp_t\n",
    "y = sp.cos(sp_t)\n",
    "\n",
    "quad_rule = gauss_rule(8)\n",
    "S = panelize_symbolic_surface(sp_t, x, y, quad_rule, n_panels=4)\n",
    "print(\n",
    "    f\"The surface has {S.n_panels} panels with {S.panel_order} quadrature points per panel and {S.n_pts} quadrature points total.\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(S.pts[:, 0], S.pts[:, 1])\n",
    "plt.quiver(\n",
    "    S.pts[:, 0],\n",
    "    S.pts[:, 1],\n",
    "    S.normals[:, 0],\n",
    "    S.normals[:, 1],\n",
    "    S.jacobians,\n",
    "    scale=20,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.axis(\"equal\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809d1bb",
   "metadata": {},
   "source": [
    "## Barycentric Lagrange interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6bbc63",
   "metadata": {},
   "source": [
    "When we compute a boundary integral, there are two sources of error: \n",
    "1. the surface approximation error\n",
    "2. the quadrature error.\n",
    "\n",
    "We've been focusing so far on the quadrature error because it can be reduced dramatically with better algorithms, especially in the singular or near-singular case. The surface approximation error is handled simply through using a higher resolution approximation to the surface -- for example, representing a circle with 100 points instead of 50. However, in the rest of this section it will be nice to be able to hold the surface approximation error constant while reducing the quadrature error to zero. But, in the integration techniques we have been using, the quadrature error and the surface error are inextricably linked. When we increase from using 50 to 100 points to integrate a function over a circle, we have been improving both the surface approximation and also using a more accurate quadrature rule.\n",
    "\n",
    "To separate the two components, we'll interpolate points from a low order surface approximation in order to calculate the locations of quadrature points for a higher order integral approximation. To make the difference more concrete... Before, we would calculate new point locations for a circle by calculating $(cos \\theta, sin \\theta)$. Now, we will calculate the new point from a polynomial interpolation of the $n$ existing points $\\sum_{i}^n c_i p_i(x)$. In some sense, this is also more realistic. In a real-world application, we normally have a data-derived surface representation that we can't improve. On the other hand, even in that real world setting, we *can* add more quadrature points by interpolating on that surface. But adding more quadrature points won't make the surface itself any more accurate.\n",
    "\n",
    "We won't need to worry directly about interpolation since it's handled internally to `integrate_term` but it's a pretty fundamental component of the numerical methods being used here, so I figured it'd be good to introduce.\n",
    "\n",
    "We're going to use [barycentric Lagrange interpolation](https://people.maths.ox.ac.uk/trefethen/barycentric.pdf){cite:p}`berrutBarycentricLagrangeInterpolation2004`. I strongly recommend that paper if you've never run into barycentric Lagrange interpolation before!\n",
    "\n",
    "Since we're integrating and interpolating functions on a finite interval, we'll use a Gaussian quadrature rule for doing the quadrature. As a result, we will be interpolating from the Gauss-Legendre points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_rule(n):\n",
    "    \"\"\"\n",
    "    The n-point gauss quadrature rule on [-1, 1].\n",
    "    Returns tuple of (points, weights)\n",
    "    \"\"\"\n",
    "    k = np.arange(1.0, n)\n",
    "    a_band = np.zeros((2, n))\n",
    "    a_band[1, 0 : (n - 1)] = k / np.sqrt(4 * k * k - 1)  # noqa: E203\n",
    "    x, V = scipy.linalg.eig_banded(a_band, lower=True)\n",
    "    w = 2 * np.real(np.power(V[0, :], 2))\n",
    "    return x, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb335a7",
   "metadata": {},
   "source": [
    "Below is a little check to make sure our interpolation snippet is working correctly. We interpolate $sin(5x)$ (plotted with a solid black line) on a grid of 7 points (blue dots) and plot the resulting approximate function (red dashes line). This isn't a rigorous check, but it is working! Convergence is very fast if we increase the interpolation order, but I've left out a demonstration of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee004e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate\n",
    "\n",
    "# First, form the interpolating polynomial\n",
    "qx, qw = gauss_rule(7)\n",
    "fqx = np.sin(5 * qx)\n",
    "I = scipy.interpolate.BarycentricInterpolator(qx, fqx)\n",
    "\n",
    "# Then, evaluate the polynomial at a bunch of points for plotting.\n",
    "xs = np.linspace(-1, 1, 200)\n",
    "v = I(xs)\n",
    "\n",
    "plt.plot(qx, fqx, \"bo\", markersize=10)\n",
    "plt.plot(xs, v, \"r--\")\n",
    "plt.plot(xs, np.sin(5 * xs), \"k-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e911e06",
   "metadata": {},
   "source": [
    "## Hypersingular stress integrals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a1d71",
   "metadata": {},
   "source": [
    "The second missing piece is a set of tools for computing stresses in the volume given an input fault slip. Continuing in the antiplane strain setting, what we want is to compute the gradient of displacement times the shear modulus.\n",
    "\n",
    "\\begin{equation}\n",
    "(\\sigma_{xz}, \\sigma_{yz}) = (\\mu \\frac{\\partial \\phi}{\\partial x}, \\mu \\frac{\\partial \\phi}{\\partial y})\n",
    "\\end{equation}\n",
    "\n",
    "The hypersingular integral will computes $\\sigma_{xz}$ and $\\sigma_{yz}$ for us given the source slip distribution. Since, we already built all the components of a QBX algorithm for the double layer case, we can now just write the naive integrator for a new kernel and everything works perfectly. In the cell below, I've implemented a naive integrator for the hypersingular integral.\n",
    "\n",
    "```{margin}\n",
    "As a reminder, By \"naive integrator\", I just mean the non-QBX integration function that would be the equivalent of the `double_layer_matrix` function from the previous section. This is equivalent to \"directly\" integrating the integral equation. (it might be good to be more consistent about these terms?)\n",
    "```\n",
    "\n",
    "Why is this kernel called \"hypersingular\"? Because the kernel behaves like $O(\\frac{1}{r^2})$ in 2D. This makes the integral especially difficult for many traditional integration methods. As you'll see below, this is not a barrier for QBX and we are able to calculate the integral extremely accurately even right on the surface.\n",
    "\n",
    "```{note}\n",
    "A quick summary of the common types of singular integrand behavior:\n",
    "- **Weakly singular**: the integrand behaves like $O(log(r))$ in 2D. When we integrate a weakly singular integral over a surface, the integral is well-defined in the normal sense.\n",
    "- **Strongly singular**: The double layer potential is strongly singular. The integrand behaves like $O(\\frac{1}{r})$ in 2D. When we integrate a strongly singular integral over a surface, the integral must be defined in the Cauchy principal value sense. Otherwise, the integral is divergent (more or less equal to infinity). However, when the integral is computed as an interior limit, the value is well defined because no integral along the limiting path is ever actually singular.\n",
    "- **Hypersingular**: The integrand behaves like $O(\\frac{1}{r^2})$. The integral is so divergent that it's not even well-defined in the Cauchy principal value sense. Like strongly singular integrals, computing as an interior limit works out well.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypersingular(obs_pts, src_pts, src_normals):\n",
    "    dx = obs_pts[:, 0, None] - src_pts[None, :, 0]\n",
    "    dy = obs_pts[:, 1, None] - src_pts[None, :, 1]\n",
    "    r2 = dx ** 2 + dy ** 2\n",
    "\n",
    "    A = 2 * (dx * src_normals[None, :, 0] + dy * src_normals[None, :, 1]) / r2\n",
    "    C = -1.0 / (2 * np.pi * r2)\n",
    "\n",
    "    out = np.empty((obs_pts.shape[0], 2, src_pts.shape[0]))\n",
    "\n",
    "    # The definition of the hypersingular kernel.\n",
    "    # unscaled sigma_xz component\n",
    "    out[:, 0, :] = src_normals[None, :, 0] - A * dx\n",
    "    # unscaled sigma_xz component\n",
    "    out[:, 1, :] = src_normals[None, :, 1] - A * dy\n",
    "\n",
    "    return out * C[:, None, :, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8b1b5",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "By leaving out the shear modulus, I'm implicitly assuming that $\\mu = 1$. You can just imagine that we're solving a nondimensionalized version of the problem. This is quite common because scaling the displacement and stress to lie in a similar range of values can improve the numerical conditioning of some problems.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a15f55",
   "metadata": {},
   "source": [
    "## General purpose interior evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1a156",
   "metadata": {},
   "source": [
    "Next, I'm going to combine the ideas of the last section into a single function that will calculate a surface integral for an arbitrary set of observation points. To be precise, we will compute:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\int_{S} K(x, y) \\phi(y)\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ corresponds to `obs_pts`, $S$ corresponds to `surface`, $K(x,y)$ corresponds to `kernel`, and $\\phi(y)$ corresponds to `density`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba722b",
   "metadata": {},
   "source": [
    "Finally, I'll write a pretty big function that is going to produce nice figures for comparing QBX against a naive computation. The function is written to be independent of the surface and kernel function. It also accepts QBX parameters. As a reminder, `offset_mult` is a multiplier for how far off the surface the QBX expansion centers are placed. `kappa` is the upsampling rate in case we want to use a higher order quadrature for computing QBX coefficients than for representing the surface. And `qbx_p` is the order of the power series expansion. Please look through the function! Very little is new compared to the previous section. We're just applying the tools we've already built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tectosaur2 import integrate_term, pts_grid, gauss_rule, upsample\n",
    "from tectosaur2.laplace2d import double_layer, hypersingular\n",
    "def qbx_example(kernel, surface, n_panels, vmin=None, vmax=None):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    def slip_fnc(xhat):\n",
    "        # This must be zero at the endpoints for potential gradient to be finite.\n",
    "        # return np.ones_like(xhat)\n",
    "        return np.cos(xhat * np.pi) + 1.0\n",
    "\n",
    "    surface_low = panelize_symbolic_surface(*surface, gauss_rule(10), n_panels=n_panels)\n",
    "    slip_low = slip_fnc(surface_low.quad_pts)\n",
    "\n",
    "    nobs = 400\n",
    "    zoomx = [-1.5, 1.5]\n",
    "    zoomy = [-1.5, 1.5]\n",
    "    xs = np.linspace(*zoomx, nobs)\n",
    "    ys = np.linspace(*zoomy, nobs)\n",
    "    obs_pts = pts_grid(xs, ys)\n",
    "\n",
    "    low_vals = (\n",
    "        kernel.direct(obs_pts, surface_low)[:, 0, :, 0]\n",
    "        .dot(slip_low)\n",
    "        .reshape((nobs, nobs))\n",
    "    )\n",
    "\n",
    "    surface_high, interp_mat = upsample(surface_low, 5)\n",
    "    slip_high = interp_mat.dot(slip_low)\n",
    "    high_vals = (\n",
    "        kernel.direct(obs_pts, surface_high)[:, 0, :, 0]\n",
    "        .dot(slip_high)\n",
    "        .reshape((nobs, nobs))\n",
    "    )\n",
    "\n",
    "    singularities = np.array(\n",
    "        [\n",
    "            [surface[1].subs(surface[0], -1), surface[2].subs(surface[0], -1)],\n",
    "            [surface[1].subs(surface[0], 1), surface[2].subs(surface[0], 1)],\n",
    "        ]\n",
    "    )\n",
    "    qbx_vals = (\n",
    "        integrate_term(\n",
    "            kernel, obs_pts, surface_low, tol=1e-10, singularities=singularities\n",
    "        )[:, 0, :, 0]\n",
    "        .dot(slip_low)\n",
    "        .reshape((nobs, nobs))\n",
    "    )\n",
    "\n",
    "    if vmin is None:\n",
    "        vmin = -1.0\n",
    "    if vmax is None:\n",
    "        vmax = 1.0\n",
    "    levels = np.linspace(vmin, vmax, 11)\n",
    "\n",
    "    obsx = obs_pts[:, 0].reshape((nobs, nobs))\n",
    "    obsy = obs_pts[:, 1].reshape((nobs, nobs))\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.title(\"Naive solution\")\n",
    "    cntf = plt.contourf(obsx, obsy, low_vals, levels=levels, extend=\"both\")\n",
    "    plt.contour(\n",
    "        obsx,\n",
    "        obsy,\n",
    "        low_vals,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"-\",\n",
    "        linewidths=0.5,\n",
    "        levels=levels,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    plt.plot(surface_high.pts[:, 0], surface_high.pts[:, 1], \"k-\", linewidth=1.5)\n",
    "    plt.xlim(zoomx)\n",
    "    plt.ylim(zoomy)\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.title(\"Upsampled naive solution\")\n",
    "    cntf = plt.contourf(obsx, obsy, high_vals, levels=levels, extend=\"both\")\n",
    "    plt.contour(\n",
    "        obsx,\n",
    "        obsy,\n",
    "        high_vals,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"-\",\n",
    "        linewidths=0.5,\n",
    "        levels=levels,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    plt.colorbar(cntf)\n",
    "    plt.plot(surface_high.pts[:, 0], surface_high.pts[:, 1], \"k-\", linewidth=1.5)\n",
    "    plt.xlim(zoomx)\n",
    "    plt.ylim(zoomy)\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.title(\"Naive error\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        logerror = np.log10(np.abs(low_vals - high_vals))\n",
    "    logerror[np.isinf(logerror)] = -12.0\n",
    "    cntf = plt.contourf(\n",
    "        obsx, obsy, logerror, levels=np.linspace(-12, 0, 13), extend=\"both\"\n",
    "    )\n",
    "    plt.contour(\n",
    "        obsx,\n",
    "        obsy,\n",
    "        logerror,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"-\",\n",
    "        linewidths=0.5,\n",
    "        levels=np.linspace(-12, 0, 13),\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    cb = plt.colorbar(cntf)\n",
    "    cb.set_label(r\"$\\log_{10}(|\\hat{u} - \\hat{u}_{\\textrm{naive}}|)$\", fontsize=14)\n",
    "    plt.plot(surface_high.pts[:, 0], surface_high.pts[:, 1], \"k-\", linewidth=1.5)\n",
    "    plt.xlim(zoomx)\n",
    "    plt.ylim(zoomy)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.title(\"QBX solution\")\n",
    "    cntf = plt.contourf(obsx, obsy, qbx_vals, levels=levels, extend=\"both\")\n",
    "    plt.contour(\n",
    "        obsx,\n",
    "        obsy,\n",
    "        qbx_vals,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"-\",\n",
    "        linewidths=0.5,\n",
    "        levels=levels,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    plt.plot(surface_high.pts[:, 0], surface_high.pts[:, 1], \"k-\", linewidth=1.5)\n",
    "    plt.xlim(zoomx)\n",
    "    plt.ylim(zoomy)\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.title(\"Upsampled naive solution\")\n",
    "    cntf = plt.contourf(obsx, obsy, high_vals, levels=levels, extend=\"both\")\n",
    "    plt.contour(\n",
    "        obsx,\n",
    "        obsy,\n",
    "        high_vals,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"-\",\n",
    "        linewidths=0.5,\n",
    "        levels=levels,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    plt.colorbar(cntf)\n",
    "    plt.plot(surface_high.pts[:, 0], surface_high.pts[:, 1], \"k-\", linewidth=1.5)\n",
    "    plt.xlim(zoomx)\n",
    "    plt.ylim(zoomy)\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.title(\"Upsampled naive error\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        logerror = np.log10(np.abs(qbx_vals - high_vals))\n",
    "    logerror[np.isinf(logerror)] = -12.0\n",
    "    cntf = plt.contourf(\n",
    "        obsx, obsy, logerror, levels=np.linspace(-12, 0, 13), extend=\"both\"\n",
    "    )\n",
    "    plt.contour(\n",
    "        obsx,\n",
    "        obsy,\n",
    "        logerror,\n",
    "        colors=\"k\",\n",
    "        linestyles=\"-\",\n",
    "        linewidths=0.5,\n",
    "        levels=np.linspace(-12, 0, 13),\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    cb = plt.colorbar(cntf)\n",
    "    cb.set_label(r\"$\\log_{10}(|\\hat{u} - \\hat{u}_{\\textrm{QBX}}|)$\", fontsize=14)\n",
    "    plt.plot(surface_high.pts[:, 0], surface_high.pts[:, 1], \"k-\", linewidth=1.5)\n",
    "    plt.xlim(zoomx)\n",
    "    plt.ylim(zoomy)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa2bec",
   "metadata": {},
   "source": [
    "## Displacement from a line source\n",
    "\n",
    "For the remainder of this part, we'll compute displacements and stresses for a three simple geometries of increasing difficulty.\n",
    "\n",
    "For each plot, there will be a geometry summary that's just a black line showing the location of the \"fault\" and red dots showing the location of the QBX expansions.\n",
    "\n",
    "Then, we'll plot three solutions: the naive solution, a high accuracy naive solution (from 2000 points) and a QBX solution. We'll also plot the $\\log_{10}$ error for both the naive and QBX solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qbx_example(double_layer, (sp_t, sp_t, 0*sp_t), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45174511",
   "metadata": {},
   "source": [
    "## Stress from a line source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qbx_example(hypersingular, (sp_t, sp_t, 0*sp_t), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c9fa6",
   "metadata": {},
   "source": [
    "## Displacement from an arc source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_t = sp.var('t')\n",
    "theta = 0.5 * sp_t * sp.pi + 0.5 * sp.pi\n",
    "x = sp.cos(theta)\n",
    "y = sp.sin(theta) - 0.5\n",
    "arc = (sp_t, x, y)\n",
    "qbx_example(double_layer, arc, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5cf22",
   "metadata": {},
   "source": [
    "## Stress from an arc source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3530252",
   "metadata": {},
   "outputs": [],
   "source": [
    "qbx_example(hypersingular, arc, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807e4fb",
   "metadata": {},
   "source": [
    "## Displacement from a challenging wavy source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ddc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_t = sp.var('t')\n",
    "x, y = sp_t, sp.sin((sp_t + 1) * 2 * sp.pi)\n",
    "wavy = (sp_t, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qbx_example(double_layer, wavy, n_panels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4787ad",
   "metadata": {},
   "source": [
    "## Stress from a challenging wavy source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710763ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qbx_example(hypersingular, wavy, 30)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
